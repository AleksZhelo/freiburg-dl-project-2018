Batch size observations:
    a smaller value seems to work better

Implemented early stopping
    average cv loss is about 0.045
    actually with high enough patience it seems just early stopping is already very good,
    about 0.012

Batch normalization:
	expected an improvement in performance, but (with early stopping) is worse
	loss worsened to about 0.035

Testing normalized (zero mean and unit variance in each column):
    more or less no difference when using early stopping? weird
    even worse with batch norm: about 0.05

    disregard all above, helps by about an order of magnitude
    dues to a typo was always on

Testing L1 regularization:
    didn't expect an improvement, but actually lowered the loss to about 0.01
    overall seems to be the best so far, with loss of ~0.00622

Tried using ELU instead of ReLU:
    didn't know what to expect, but suddenly a large improvement
    loss about 0.0095 regardless of normalization
    using ELU the L1 regularization no longer makes an improvement (from my experiments)

Dropout:
    overall not useful
    try on last layer only

DeCov:
    expect to be on par with dropout

Exponential learning rate decay:
    don't expect an improvement with RMSProp, since it scales the learning rate internally already

export PATH=/home/zheloo/miniconda3/bin:$PATH
